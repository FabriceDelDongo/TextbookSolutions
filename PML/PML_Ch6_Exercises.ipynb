{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6e0702-46c4-40cb-8ae2-fe7f6a12ea8e",
   "metadata": {},
   "source": [
    "### Probabilistic Machine Learning (Book 1)\n",
    "\n",
    "Kevin P Murphy\n",
    "\n",
    "#### Exercises, Chapter 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0eba10af-09fa-429c-b04f-64eaa2bda954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df32b3-9935-4363-9ea3-358e7166d745",
   "metadata": {},
   "source": [
    "**Exercise 6.1** [Expressing mutual information in terms of entropies *] \n",
    "\n",
    "Prove the following identities:\n",
    "\n",
    "$I(X;Y) = H(X)−H(X|Y) = H(Y)−H(Y|X)$\n",
    "\n",
    "and\n",
    "\n",
    "$H(X,Y) = H(X|Y)+H(Y|X)+I(X;Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6766551b-f57d-4ff5-896c-ea5e3f29b200",
   "metadata": {},
   "source": [
    "Recalling the definition of conditional entropy:\n",
    "\n",
    "$\\mathbb{H}(Y|X) = \\mathbb{E}_{p(X)}[\\mathbb{H}(p(Y|X))] = \\sum_{x \\in \\mathcal{X}}p(x)\\left(-\\sum_{y \\in \\mathcal{Y}}p(y|x)\\log p(y|x)\\right),$\n",
    "\n",
    "and the definition of mutual information:\n",
    "\n",
    "$\\mathbb{I}(X; Y) = \\sum_{y \\in \\mathcal{Y}}\\sum_{x \\in \\mathcal{X}}p(x,y)\\log \\frac{p(x,y)}{p(x)p(y)},$\n",
    "\n",
    "we see that\n",
    "\n",
    "$\\mathbb{I}(X; Y) = \\sum_{y \\in \\mathcal{Y}}\\sum_{x \\in \\mathcal{X}}p(x,y)\\log \\frac{p(x,y)}{p(x)} - \\sum_{y \\in \\mathcal{Y}}\\sum_{x \\in \\mathcal{X}}p(x,y)\\log p(y)$\n",
    "\n",
    "$= \\sum_{y \\in \\mathcal{Y}}\\sum_{x \\in \\mathcal{X}}p(x,y)\\log p(y|x) - \\sum_{y \\in \\mathcal{Y}}\\sum_{x \\in \\mathcal{X}}p(x,y)\\log p(y)$\n",
    "\n",
    "$= \\sum_{y \\in \\mathcal{Y}}\\sum_{x \\in \\mathcal{X}}p(y|x)p(x)\\log p(y|x) - \\sum_{y \\in \\mathcal{Y}}\\sum_{x \\in \\mathcal{X}}p(x,y)\\log p(y)$\n",
    "\n",
    "$= \\sum_{x \\in \\mathcal{X}}p(x)\\left(\\sum_{y \\in \\mathcal{Y}}p(y|x)\\log p(y|x)\\right) - \\sum_{y \\in \\mathcal{Y}}\\left(\\sum_{x \\in \\mathcal{X}}p(x,y)\\right)\\log p(y)$\n",
    "\n",
    "$= - \\mathbb{E}_{p(X)}\\left[\\mathbb{H}(p(Y|X))\\right] - \\sum_{y \\in \\mathcal{Y}}p(y)\\log p(y)$\n",
    "\n",
    "$= -\\mathbb{H}(Y|X) + \\mathbb{H}(Y),$\n",
    "\n",
    "as required for the first identity.\n",
    "\n",
    "The second identity follows from the \"Chain Rule for Entropy:\"\n",
    "\n",
    "$\\mathbb{H}(X,Y) = \\mathbb{H}(X) + \\mathbb{H}(Y|X)$\n",
    "\n",
    "and from the first identity $I(X;Y) = H(X)−H(X|Y)$:\n",
    "\n",
    "$\\mathbb{H}(X,Y) = \\mathbb{H}(Y|X) + \\mathbb{H}(X)$\n",
    "\n",
    "$= \\mathbb{H}(Y|X) + I(X;Y) + H(X|Y).$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989fa3e8-4d09-4815-b9d2-8e9f0a9af678",
   "metadata": {},
   "source": [
    "**Exercise 6.2** [Relationship between D(p||q) and χ2 statistic] (Source: [CT91, Q12.2].)\n",
    "\n",
    "Show that, if $p(x) \\approx q(x)$, then \n",
    "\n",
    "$\\text{KL}(p||q) \\approx \\frac{1}{2}\\chi^2$\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "$\\chi^2 = \\sum_{x}\\frac{(p(x) − q(x))^2}{q(x)}$\n",
    "\n",
    "Hint: write\n",
    "\n",
    "$p(x) = \\Delta(x) + q(x)$\n",
    "\n",
    "$\\frac{p(x)}{q(x)} = 1 + \\frac{\\Delta(x)}{q(x)}$\n",
    "\n",
    "and use the Taylor series expansion for $\\log(1 + x):$ \n",
    "\n",
    "$\\log(1 + x) = x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\frac{x^4}{4} \\cdots$\n",
    "\n",
    "for $−1 \\lt x \\leq 1.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5b1ee-5c53-4d33-91c8-5a2acbd7ad8b",
   "metadata": {},
   "source": [
    "The K-L Divergence is defined as:\n",
    "\n",
    "$\\mathbb{KL}(p||q) = \\sum_{x}p(x)\\log\\frac{p(x)}{q(x)}.$\n",
    "\n",
    "We have \n",
    "\n",
    "$p(x) = \\Delta(x) + q(x),$\n",
    "\n",
    "where it is understood that $\\Delta(x)$ is small, i.e., that $p(x)$ and $q(x)$ are \"close.\"\n",
    "\n",
    "We immediately have that \n",
    "\n",
    "$\\frac{p(x)}{q(x)} = 1 + \\frac{\\Delta(x)}{q(x)},$\n",
    "\n",
    "so that we can write the K-L Diveregence as:\n",
    "\n",
    "$\\mathbb{KL}(p||q) = \\sum_{x}(\\Delta(x) + q(x))\\log(1 + \\frac{\\Delta(x)}{q(x)}),$\n",
    "\n",
    "keeping only up to the quadratic term in the T.S. expansion of $\\log(1+x),$\n",
    "\n",
    "$\\mathbb{KL}(p||q) = \\sum_{x}(\\Delta(x) + q(x))(\\frac{\\Delta(x)}{q(x)} - \\frac{1}{2}\\frac{\\Delta^2(x)}{q^2(x)})$\n",
    "\n",
    "$=\\sum_x \\frac{1}{q(x)}\\left(\\Delta^2(x) - \\frac{1}{2}\\Delta^2(x) + q(x)\\Delta(x) - \\frac{1}{2}\\frac{\\Delta^3(x)}{q(x)} \\right) \\approx \\sum_x \\frac{1}{q(x)}\\left(\\frac{\\Delta^2(x)}{2}\\right),$\n",
    "\n",
    "where we have assumed $\\frac{\\Delta^3(x)}{q(x)} \\approx 0,$ and that\n",
    "\n",
    "$\\Delta(x)q(x) = p(x)q(x) - q(x)^2 \\approx q(x)^2 - q(x)^2 = 0.$\n",
    "\n",
    "Hence\n",
    "\n",
    "$\\mathbb{KL}(p||q) \\approx \\frac{1}{2} \\sum_x \\frac{1}{q(x)}\\Delta^2(x) = \\frac{1}{2} \\sum_x \\frac{(p(x)-q(x))^2}{q(x)} = \\frac{1}{2}\\chi^2.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c171a1-083e-428a-8af0-f2d58d64c843",
   "metadata": {},
   "source": [
    "**Exercise 6.3** [Fun with entropies *]\n",
    "(Source: Mackay.) \n",
    "\n",
    "Consider the joint distribution $p(X, Y)$ \n",
    "\n",
    "                 x\n",
    "           1    2    3    4\n",
    "       1  1/8  1/16  1/32  1/32\n",
    "     y 2  1/16 1/8   1/32  1/32\n",
    "       3  1/16 1/16  1/16  1/16\n",
    "       4  1/4   0     0     0\n",
    "    \n",
    "a. What is the joint entropy $H (X, Y )$?\n",
    "\n",
    "b. What are the marginal entropies $H(X)$ and $H(Y )$?\n",
    "\n",
    "c. The entropy of $X$ conditioned on a specific value of $y$ is defined as \n",
    "\n",
    "$H(X|Y=y)=-\\sum_x p(x|y)\\log_2 p(x|y)$\n",
    "\n",
    "Compute $H(X|y)$ for each value of $y$. Does the posterior entropy on $X$ ever increase given an observation of $Y$?\n",
    "\n",
    "d. The conditional entropy is defined as\n",
    "\n",
    "$H(X|Y ) = \\sum_y p(y)H(X|Y = y)$\n",
    "\n",
    "Compute this. Does the posterior entropy on $X$ increase or decrease when averaged over the possible values of $Y$ ?\n",
    "\n",
    "e. What is the mutual information between $X$ and $Y$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6eea1e-aeb7-4de5-81f2-bd037bb564fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'X=1': [1/8, 1/16, 1/16, 1/4],\n",
    "                   'X=2': [1/16, 1/8, 1/16, 0],\n",
    "                   'X=3': [1/32, 1/32, 1/16, 0],\n",
    "                   'X=4': [1/32, 1/32, 1/16, 0],},\n",
    "                 index=['Y=1', 'Y=2', 'Y=3', 'Y=4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b958e6-5162-40e5-85cd-a0ef685832ea",
   "metadata": {},
   "source": [
    "a.\n",
    "\n",
    "The joint entropy $H(X,Y)$ is given by:\n",
    "\n",
    "$H(X, Y) = - \\sum_{x, y}p(x, y)\\log_2 p(x, y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c640fccc-7b90-46c3-a8e1-ae97a046effb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joint entropy H(X, Y) = 3.3750 bits\n"
     ]
    }
   ],
   "source": [
    "H = 0\n",
    "mat = df.values\n",
    "for ii in range(mat.shape[0]):\n",
    "    for ij in range(mat.shape[1]):\n",
    "        if mat[ii, ij] > 0:\n",
    "            H += - mat[ii, ij]*math.log(mat[ii, ij], 2)\n",
    "        \n",
    "print(f\"The joint entropy H(X, Y) = {H:.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7bc60d-9a26-4daa-8667-80926f1aeb76",
   "metadata": {},
   "source": [
    "b.\n",
    "\n",
    "The marginal entropy $H(X)$ is given by\n",
    "\n",
    "$H(X) = - \\sum_{x}p(x) \\log_2 p(x),$\n",
    "\n",
    "where\n",
    "\n",
    "$p(x=X) = \\sum_{y} p(x=X, y).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b1d2384-10b9-4297-b0df-ef7467dca24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pX = df.sum(axis=0)\n",
    "pY = df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa54097f-d033-4a20-88f7-f61f311eb53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=1    0.500\n",
      "X=2    0.250\n",
      "X=3    0.125\n",
      "X=4    0.125\n",
      "dtype: float64\n",
      "Y=1    0.25\n",
      "Y=2    0.25\n",
      "Y=3    0.25\n",
      "Y=4    0.25\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(pX)\n",
    "print(pY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c064ec34-4cca-4c00-943e-f71c1ed753ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The marginal entropy H(X) = 1.7500 bits\n",
      "The marginal entropy H(Y) = 2.0000 bits\n"
     ]
    }
   ],
   "source": [
    "HX = 0\n",
    "for px in pX.values:\n",
    "    if px > 0:\n",
    "        HX += - px * math.log(px, 2)\n",
    "HY = 0\n",
    "for py in pY.values:\n",
    "    if py > 0:\n",
    "        HY += - py * math.log(py, 2)\n",
    "        \n",
    "print(f\"The marginal entropy H(X) = {HX:.4f} bits\")\n",
    "print(f\"The marginal entropy H(Y) = {HY:.4f} bits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7957aed3-b134-4a23-a749-352681db63fd",
   "metadata": {},
   "source": [
    "c.\n",
    "\n",
    "We calculate the conditional probabilitites $p(X|Y=y)$ using $p(X|Y=y) = \\frac{p(X,Y)}{P(Y=y)}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13bd90ad-c6f8-4752-9351-8da358d92f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pXcondY = df.values / pY.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ff4ea33-0240-4a84-9868-ac3de7f1fadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5  , 0.25 , 0.125, 0.125],\n",
       "       [0.25 , 0.5  , 0.125, 0.125],\n",
       "       [0.25 , 0.25 , 0.25 , 0.25 ],\n",
       "       [1.   , 0.   , 0.   , 0.   ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pXcondY # rows correspond to y = 1, 2, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a14b17d1-d2ef-4e3c-8fab-bbc66b836c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X|Y=1) = 1.7500\n",
      "H(X|Y=2) = 1.7500\n",
      "H(X|Y=3) = 2.0000\n",
      "H(X|Y=4) = 0.0000\n"
     ]
    }
   ],
   "source": [
    "for ii in range(4):\n",
    "    y = ii+1\n",
    "    H = 0\n",
    "    for ij in range(4):\n",
    "        if pXcondY[ii, ij] > 0:\n",
    "            H += -pXcondY[ii, ij] * math.log(pXcondY[ii, ij], 2)\n",
    "    print(f\"H(X|Y={y}) = {H:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9175a5-9b2e-41df-a30c-2aad83634fef",
   "metadata": {},
   "source": [
    "The marginal entropy for $X,$ calculated above as $H(X) = 1.75$ increases to a posterior entropy $H(X|Y=3) = 2.0$ after an observation of $Y = 3.$  This is because the distribution $p(X|Y=3)$ is uniform.  Conversely, the posterior entropy $H(X|Y=4) = 0,$ since $p(X|Y=4)$ is degenerate (all its mass is on $X=1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c310ba-be43-4379-b062-34183def37cc",
   "metadata": {},
   "source": [
    "d. \n",
    "\n",
    "The conditional entropy $H(X|Y)$ (note that this is different to the posterior entropy for $X$ after an observation of a specific value of $Y$ calculated in part c; it is the expectation of the posterior entropy over $Y$) is computed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3dd0291-e3e8-4dbe-8ca7-e658f8432e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Y=1    0.25\n",
       "Y=2    0.25\n",
       "Y=3    0.25\n",
       "Y=4    0.25\n",
       "dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "148a1d06-0d12-4b30-a456-f35efbb0c1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X|Y) = 1.3750\n"
     ]
    }
   ],
   "source": [
    "H = 0\n",
    "for ii in range(4): # loop over p(Y)\n",
    "    H_ = 0\n",
    "    for ij in range(4): # loop over p(X)\n",
    "        if pXcondY[ii, ij] > 0:\n",
    "            H_ += -pXcondY[ii, ij] * math.log(pXcondY[ii, ij], 2)\n",
    "    H += pY[ii] * H_\n",
    "print(f\"H(X|Y) = {H:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664b4977-dabf-47fc-bd7b-c1ee8411c42d",
   "metadata": {},
   "source": [
    "The conditional entropy $H(X|Y)$ is reduced from the marginal entropy $H(X).$  For a specific observation of $Y,$ we may find the posterior entropy increase, but for the conditional entropy, defined as the average posterior entropy over all states of $Y,$ the conditional entropy can only decrease with respect to the marginal value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d177f44a-e1dd-40b3-868d-99cee611f226",
   "metadata": {},
   "source": [
    "e.\n",
    "\n",
    "The mutual information between $X$ and $Y$ can be calculated in three different ways using the quantities calculated above:\n",
    "\n",
    "* The difference between the marginal and conditional entropies for either variable\n",
    "* The difference between the joint entropy and the sum of the conditional entropies\n",
    "* The difference between the sum of the marginal entropies and the joint entropy\n",
    "\n",
    "In order to verify the computation using all three methods, we first calculate the conditional entropy $H(Y|X)$ using the same approach as that used to compute $p(X|Y).$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57490de7-c99f-465e-aa05-c9fd8eea6d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(Y|X) = 1.6250\n"
     ]
    }
   ],
   "source": [
    "pYcondX = df.values / pX.values\n",
    "\n",
    "H = 0\n",
    "for ij in range(4): # loop over p(X) (columns)\n",
    "    H_ = 0\n",
    "    for ii in range(4): # loop over p(Y) (rows)\n",
    "        if pYcondX[ii, ij] > 0:\n",
    "            H_ += -pYcondX[ii, ij] * math.log(pYcondX[ii, ij], 2)\n",
    "    H += pX[ij] * H_\n",
    "print(f\"H(Y|X) = {H:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17d2b72e-dd25-4ca3-8e32-d58854957299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25 , 0.25 , 0.25 , 0.25 ],\n",
       "       [0.125, 0.5  , 0.25 , 0.25 ],\n",
       "       [0.125, 0.25 , 0.5  , 0.5  ],\n",
       "       [0.5  , 0.   , 0.   , 0.   ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pYcondX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "095d5884-4e6d-409d-b381-3d214eed5b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I(X;Y) = 0.3750\t\t(H(X) - H(X|Y))\n",
      "I(X;Y) = 0.3750\t\t(H(Y) - H(Y|X))\n",
      "I(X;Y) = 0.3750\t\t(H(X, Y) - (H(X|Y) + H(Y|X))\n",
      "I(X;Y) = 0.3750\t\t(H(X) + H(Y) - H(X,Y))\n"
     ]
    }
   ],
   "source": [
    "print(f\"I(X;Y) = {(1.75 - 1.375):.4f}\\t\\t(H(X) - H(X|Y))\")\n",
    "print(f\"I(X;Y) = {(2.00 - 1.625):.4f}\\t\\t(H(Y) - H(Y|X))\")\n",
    "print(f\"I(X;Y) = {(3.375 - (1.375 + 1.625)):.4f}\\t\\t(H(X, Y) - (H(X|Y) + H(Y|X))\")\n",
    "print(f\"I(X;Y) = {(1.75 + 2.00 - 3.375):.4f}\\t\\t(H(X) + H(Y) - H(X,Y))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95828b78-78a1-43c4-a71b-96521f889b70",
   "metadata": {},
   "source": [
    "**Exercise 6.4** [Forwards vs reverse KL divergence]\n",
    "(Source: Exercise 33.7 of [Mac03].) \n",
    "\n",
    "Consider a factored approximation $q(x, y) = q(x)q(y)$ to a joint distribution $p(x, y)$. Show that to minimize the forwards KL $\\mathbb{KL}(p||q)$ we should set $q(x) = p(x)$ and $q(y) = p(y)$, i.e., the optimal approximation is a product of marginals.\n",
    "\n",
    "Now consider the following joint distribution, where the rows represent $y$ and the columns $x$. \n",
    "\n",
    "           1   2   3   4\n",
    "        1 1/8 1/8  0   0\n",
    "        2 1/8 1/8  0   0\n",
    "        3  0   0  1/4  0\n",
    "        4  0   0   0  1/4\n",
    "\n",
    "Show that the reverse KL $\\mathbb{KL}(q||p)$ for this $p$ has three distinct minima. Identify those minima and evaluate $\\mathbb{KL}(q||p)$ at each of them. What is the value of $\\mathbb{KL}(q||p)$ if we set $q(x, y) = p(x)p(y)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e152452-3501-42d8-a985-d0ee2a1b8187",
   "metadata": {},
   "source": [
    "We have \n",
    "\n",
    "$\\mathbb{KL}(p(x, y)||q(x)q(y)) = \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x, y) \\log \\frac{p(x, y)}{q(x)q(y)}$\n",
    "\n",
    "$= \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x, y) \\log p(x, y) - \n",
    "   \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x, y) \\log q(x) - \n",
    "   \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x, y) \\log q(y),$\n",
    "   \n",
    "where the first term is unaffected by the choice of $q(x), q(y)$, and the second and third terms, are maximized respectively by:\n",
    "\n",
    "$q^*(x) = \\text{argmax}_{q(x)}\\sum_{x \\in \\mathcal{X}} \\left[\\sum_{y \\in \\mathcal{Y}} p(x, y)\\right] \\log q(x) = \\text{argmax}_{q(x)}\\mathbb{E}[\\log q(x)] = p(x),$\n",
    "\n",
    "$q^*(y) = \\text{argmax}_{q(y)}\\sum_{y \\in \\mathcal{Y}} \\left[\\sum_{x \\in \\mathcal{X}} p(x, y)\\right] \\log q(y) = \\text{argmax}_{q(y)}\\mathbb{E}[\\log q(y)] = p(y).$\n",
    "\n",
    "Hence choosing $q(x), q(y)$ to be the marginals associated with the joint distribution $p(x, y)$ will lead to the lowest possible K-L divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cd27f989-3156-46ae-a02f-72f44c2df287",
   "metadata": {},
   "outputs": [],
   "source": [
    "pxy = pd.DataFrame({'X=1':[1/8, 1/8, 0, 0],\n",
    "                    'X=2':[1/8, 1/8, 0, 0],\n",
    "                    'X=3':[0,   0, 1/4, 0],\n",
    "                    'X=4':[0,   0,   0, 1/4]}, \n",
    "                  index = ['Y=1', 'Y=2', 'Y=3', 'Y=4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f50e5c43-b573-4382-a8e8-e0262d0347fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X=1</th>\n",
       "      <th>X=2</th>\n",
       "      <th>X=3</th>\n",
       "      <th>X=4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Y=1</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y=2</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y=3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y=4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       X=1    X=2   X=3   X=4\n",
       "Y=1  0.125  0.125  0.00  0.00\n",
       "Y=2  0.125  0.125  0.00  0.00\n",
       "Y=3  0.000  0.000  0.25  0.00\n",
       "Y=4  0.000  0.000  0.00  0.25"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0749d4-0d6a-4c28-8f3e-d202079524e9",
   "metadata": {},
   "source": [
    "We know that the reverse K-L:\n",
    "\n",
    "$\\mathbb{KL}(q||p) = \\int q(x)\\log \\frac{q(x)}{p(x)} dx \\text{    (continuous version)},$\n",
    "\n",
    "when minimized, will exclude all areas of the space for which $p$ is zero.\n",
    "\n",
    "We will then expect to see three minima in the K-L divergence, two of which being degenerate (with all the probability mass at $X = Y = 3$ and $X = Y = 4,$ respectively), and one being non-degenerate but having having mass evenly distributed on $X \\in \\{1, 2\\}$ and $Y \\in \\{1, 2\\}.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e111458d-69e6-44bb-9ba5-2a4f4a4b3af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_KL(p_xy, q_xy):\n",
    "    \"\"\"\n",
    "    Calculate the reverse K-L divergence for a \n",
    "    given joint target distribution df p_xy and \n",
    "    approximate distribution df q_xy\n",
    "    \"\"\"\n",
    "    assert p_xy.sum().sum() == 1\n",
    "    assert q_xy.sum().sum() == 1\n",
    "    pxy = p_xy.values\n",
    "    qxy = q_xy.values\n",
    "    assert pxy.shape == qxy.shape\n",
    "    KL = 0\n",
    "    for ii in range(pxy.shape[0]):\n",
    "        for ij in range(pxy.shape[1]):\n",
    "            if pxy[ii, ij] > 0 and qxy[ii, ij] > 0:\n",
    "                KL += qxy[ii, ij] * math.log(qxy[ii, ij]/pxy[ii, ij], 2)\n",
    "            elif pxy[ii, ij] == 0 and qxy[ii, ij] > 0: # denominator = 0\n",
    "                KL += math.inf\n",
    "    return KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a1ff05e7-ba39-4093-8059-eaa002921900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q = p would give us zero K-L Divergence:\n",
    "reverse_KL(pxy, pxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7806cd-994b-4944-945c-2b53572a5a1d",
   "metadata": {},
   "source": [
    "Degenerate minimum #1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cea85247-2e52-47e2-88e3-996f4fd73b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qxy = pd.DataFrame({'X=1':[0,   0,   0, 0],\n",
    "                    'X=2':[0,   0,   0, 0],\n",
    "                    'X=3':[0,   0,   0, 0],\n",
    "                    'X=4':[0,   0,   0, 1]}, \n",
    "                  index = ['Y=1', 'Y=2', 'Y=3', 'Y=4'])\n",
    "reverse_KL(pxy, qxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbeceb0-5455-45b5-9b0b-5487cf91d3c6",
   "metadata": {},
   "source": [
    "Degenerate minimum #2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "55efa685-63cc-429e-8ad5-e425ecbc3c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qxy = pd.DataFrame({'X=1':[0,   0,   0, 0],\n",
    "                    'X=2':[0,   0,   0, 0],\n",
    "                    'X=3':[0,   0,   1, 0],\n",
    "                    'X=4':[0,   0,   0, 0]}, \n",
    "                  index = ['Y=1', 'Y=2', 'Y=3', 'Y=4'])\n",
    "reverse_KL(pxy, qxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f4f32-c2c0-479c-abf5-f9fe1f8d395d",
   "metadata": {},
   "source": [
    "Non-degenerate minimum: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b1d6478f-17c5-406f-9bf5-e2fd9def7300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qxy = pd.DataFrame({'X=1':[1/4, 1/4, 0, 0],\n",
    "                    'X=2':[1/4, 1/4, 0, 0],\n",
    "                    'X=3':[0,   0,   0, 0],\n",
    "                    'X=4':[0,   0,   0, 0]}, \n",
    "                  index = ['Y=1', 'Y=2', 'Y=3', 'Y=4'])\n",
    "reverse_KL(pxy, qxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5c4318a0-c13e-4554-9d42-2320ba071a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qxy = pd.DataFrame({'X=1':[0,   0,   0, 0],\n",
    "                    'X=2':[0,   0,   0, 0],\n",
    "                    'X=3':[0,   0,   0.5, 0],\n",
    "                    'X=4':[0,   0,   0, 0.5]}, \n",
    "                  index = ['Y=1', 'Y=2', 'Y=3', 'Y=4'])\n",
    "reverse_KL(pxy, qxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a647ff-be93-45f5-8359-35955ce66d22",
   "metadata": {},
   "source": [
    "The K-L Divergence can be lowered from these minima by joint distributions that cover additional non-zero regions of the target distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a8b56605-7aa7-470e-8c4a-616d72a5df9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08170416594551039"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qxy = pd.DataFrame({'X=1':[1/12,   1/12,    0,   0],\n",
    "                    'X=2':[1/12,   1/12,    0,   0],\n",
    "                    'X=3':[0,         0,  1/3,   0],\n",
    "                    'X=4':[0,         0,    0, 1/3]}, \n",
    "                  index = ['Y=1', 'Y=2', 'Y=3', 'Y=4'])\n",
    "reverse_KL(pxy, qxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f2383e-57e8-467d-a391-2c9dfa89f446",
   "metadata": {},
   "source": [
    "However such distributions will often be difficult to identify in real applications.\n",
    "\n",
    "In this case setting $q(x,y)$ to be the product of the marginals $p(x)p(y)$ will lead to infinite K-L divergence, as $q(x, y)$ will have mass in regions of probability-space where $p(x, y)$ does not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "85e60b05-2492-4708-9240-74329f6e8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pX = pxy.sum(axis=0).values\n",
    "pY = pxy.sum(axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "90a6f519-4f01-4142-af21-05d3170e30f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25 0.25 0.25 0.25]\n",
      "[0.25 0.25 0.25 0.25]\n"
     ]
    }
   ],
   "source": [
    "print(pX)\n",
    "print(pY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "1cd6d155-ea87-4722-afdb-86a9c3170abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qXY = pd.DataFrame(pX.reshape(1, 4) * pY.reshape(4, 1),\n",
    "                  columns=['X=1', 'X=2', 'X=3', 'X=4'],\n",
    "                  index=['Y=1', 'Y=2', 'Y=3', 'Y=4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e5edba70-47ee-46c6-b6fd-963c22ebc558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X=1</th>\n",
       "      <th>X=2</th>\n",
       "      <th>X=3</th>\n",
       "      <th>X=4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Y=1</th>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y=2</th>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y=3</th>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y=4</th>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        X=1     X=2     X=3     X=4\n",
       "Y=1  0.0625  0.0625  0.0625  0.0625\n",
       "Y=2  0.0625  0.0625  0.0625  0.0625\n",
       "Y=3  0.0625  0.0625  0.0625  0.0625\n",
       "Y=4  0.0625  0.0625  0.0625  0.0625"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qXY # product of the marginals leads to a uniform dictribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b72a63d9-332c-45e8-88e8-c04a8b5d60c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_KL(pxy, qXY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
