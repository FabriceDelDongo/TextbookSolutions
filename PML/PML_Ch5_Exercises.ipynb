{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c6e0702-46c4-40cb-8ae2-fe7f6a12ea8e",
   "metadata": {},
   "source": [
    "### Probabilistic Machine Learning (Book 1)\n",
    "\n",
    "Kevin P Murphy\n",
    "\n",
    "#### Exercises, Chapter 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db61ffb-a4de-41c7-bbd5-c30573fc4cd3",
   "metadata": {},
   "source": [
    "**Exercise 5.1** [Reject option in classifiers]\n",
    "\n",
    "(Source: [DHS01, Q2.13].) In many classification problems one has the option either of assigning $x$ to class $j$ or, if you are too uncertain, of choosing the reject option. If the cost for rejects is less than the cost of falsely classifying the object, it may be the optimal action. Let $\\alpha_i$ mean you choose action $i$, for $i = 1 : C + 1$, where $C$ is the number of classes and $C + 1$ is the reject action. Let $Y = j$ be the true (but unknown) state of nature. Define the loss function as follows\n",
    "\n",
    "$\\lambda(\\alpha_i|Y=j)= \n",
    "\\begin{cases}\n",
    "    0        ,& \\text{if } i=j   \\\\\n",
    "    \\lambda_r,& \\text{if } i=C+1 \\\\\n",
    "    \\lambda_s,& \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "In other words, you incur $0$ loss if you correctly classify, you incur $\\lambda_r$ loss (cost) if you choose the reject option, and you incur λs loss (cost) if you make a substitution error (misclassification).\n",
    "\n",
    "a. Show that the minimum risk is obtained if we decide $Y = j$ if $p(Y =j|x)\\geq p(Y =k|x)$ for all $k$ (i.e., $j$ is the most probable class) _and_ if $p(Y = j|x) \\geq 1 − \\frac{\\lambda_r}{\\lambda_s}$ ; otherwise we decide to reject.\n",
    "\n",
    "b. Describe qualitatively what happens as $\\lambda_r/\\lambda_s$ is increased from $0$ to $1$ (i.e., the relative cost of rejection increases)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7d3d90-fdd2-4273-ac7b-5c35e8cb438d",
   "metadata": {},
   "source": [
    "a.\n",
    "\n",
    "Consider first the case where the cost of the reject action is too high for it ever to be useful, i.e., where $\\lambda_r > \\lambda_s.$\n",
    "\n",
    "In that scenario, our expected risk will be:\n",
    "\n",
    "$R(\\alpha_i|x) = \\mathbb{E}_{p(j|x)}\\lambda(\\alpha_i, j) = \\sum_{j=1,\\ldots,C}\\lambda(\\alpha_i, j)p(j|x)$\n",
    "\n",
    "$=\\lambda(\\alpha_i, j=1)p(j=1|x) + \\lambda(\\alpha_i, j=2)p(j=2|x) + \\ldots + \\lambda(\\alpha_i, j=3)p(j=C|x)$\n",
    "\n",
    "$=\\lambda_s(1-p(i = j|x)),$\n",
    "\n",
    "where $p(i = j|x)$ is the probability of making a correct classification, no matter the class.  Clearly to minimize the expected risk absent the reject option we will want to choose the class with the highest probability.\n",
    "\n",
    "Now consider the introduction of the reject option.  Assuming that we first choose the class $c$ which maximizes $p(Y=c|x)$ for some input data $x$.  We will then want to predict class $c$ for that input only provided the probability for that class $p(Y=c|x)$ is above some threshold value $p^*$.  Considering the associated risk for our classification:\n",
    "\n",
    "$\\lambda(\\alpha_i, j=c) = \\begin{cases}\n",
    "    0\\cdot p(Y=c|x)+\\lambda_s (1-p(Y=c|x)) ,& \\text{if } p(Y=c|x) \\gt p^*\\\\\n",
    "    \\lambda_r,                                 & \\text{if } p(Y=c|x) \\leq p^*,\\\\\n",
    "\\end{cases}$\n",
    "\n",
    "so that, at our threshold value,\n",
    "\n",
    "$\\lambda_s(1-p^*) = \\lambda_r,$ which gives us $p^* = 1 - \\frac{\\lambda_r}{\\lambda_s}.$\n",
    "\n",
    "Hence we will first find the class $c$ that maximizes the probability $p(Y=j|x),$ and then predict that class iff $p(Y=c|x) > 1 - \\frac{\\lambda_r}{\\lambda_s},$ otherwise we should choose the rejext option.\n",
    "\n",
    "b.\n",
    "\n",
    "When $\\frac{\\lambda_r}{\\lambda_s} = 0,$ we will always choose the reject option, since the expected loss for that action is zero, while the expected loss for any prediction that we are not absolutely certain about will be non-zero.  As $\\frac{\\lambda_r}{\\lambda_s}$ increases from zero we will start to see predictions only for high-confidence class predictions (i.e., where $p(Y=c|x) \\gt 1 - \\frac{\\lambda_r}{\\lambda_s}$), while lower confidence predictions will still be rejected.  Finally, as $\\frac{\\lambda_r}{\\lambda_s}$ increases from $0$, the reject option will become used extremely infrequently, since there will nearly always be one class above the threshold, expecially if the number of classes is small (e.g., the lowest possible confidence for a class in a three class classifier is $1/3 \\approx 33.3\\%,$ whereas the lowest possible confidence for a 100 class classifier is 1%).  At the point at which $\\frac{\\lambda_r}{\\lambda_s} > 1/C$ there will be zero chance of the reject option being employed, and the classifier will make the highest probability class prediction every time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138697d7-98ed-40b2-8133-dfc0556df9c5",
   "metadata": {},
   "source": [
    "**Exercise 5.2** [Newsvendor problem *]\n",
    "\n",
    "Consider the following classic problem in decision theory / economics. Suppose you are trying to decide how much quantity $Q$ of some product (e.g., newspapers) to buy to maximize your profits. The optimal amount will depend on how much demand $D$ you think there is for your product, as well as its cost to you $C$ and its selling price $P$. Suppose $D$ is unknown but has pdf $f(D)$ and cdf $F(D$). We can evaluate the expected profit by considering two cases: if $D$ > $Q$, then we sell all $Q$ items, and make profit $\\pi = (P −C)Q$; but if $D < Q$, we only sell $D$ items, at profit $(P − C)D$, but have wasted $C(Q − D)$ on the unsold items. So the expected profit if we buy quantity $Q$ is\n",
    "\n",
    "$E[\\pi(Q)] = \\int_Q^\\infty (P-C)Q f(D)dD + \\int_0^Q (P-C)D f(D)dD - \\int_0^Q C(Q-D) f(D)dD$\n",
    "\n",
    "Simplify this expression, and then take derivatives wrt $Q$ to show that the optimal quantity $Q^*$ (which\n",
    "maximizes the expected profit) satisfies\n",
    "\n",
    "$F(Q^*) = \\frac{P-C}{P}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a94d2d-281d-4b2a-b8f5-47549399907f",
   "metadata": {},
   "source": [
    "Integrating the expression for the expectation, using the fact that the cdf is the integral of the pdf, we have\n",
    "\n",
    "$E[\\pi(Q)] = (P-C)Q F(D) \\biggr|_Q^\\infty  + \\bigg[(P-C)D-C(Q-D)\\bigg] F(D) \\biggr|_0^Q$\n",
    "\n",
    "$= (P-C)Q(1-F(Q))  + \\bigg[(P-C)D-C(Q-D)\\bigg](F(Q)-0),$\n",
    "\n",
    "since $F(\\infty) = 1$ and $F(0) = 0$.\n",
    "\n",
    "So\n",
    "\n",
    "$E[\\pi(Q)] = F(Q)\\bigg[(P-C)D-(Q-D)C-(P-C)Q \\bigg] + (P-C)Q$\n",
    "\n",
    "$=F(Q)(PD-PQ) + PQ - CQ.$\n",
    "\n",
    "At the optimal $Q$, $Q^*,$ the derivative $\\frac{d}{dQ}(E[\\pi(Q)]) = 0,$ i.e., \n",
    "\n",
    "$\\frac{d}{dQ} \\bigg[F(Q)PD - F(Q)PQ + PQ - CQ \\bigg]_{Q=Q^*} = 0.$\n",
    "\n",
    "$\\Rightarrow F(Q^*)PD - F'(Q^*)PQ^* - PF(Q^*) + P - C = 0,$ so that\n",
    "\n",
    "$PF(Q^*) = P - C + F(Q^*)P(D - D),$\n",
    "\n",
    "since at the optimal value $Q^*$ must equal $D,$ i.e., we buy the exact amount for which there is demand, and\n",
    "\n",
    "$F(Q^*) = \\frac{P-C}{P}$\n",
    "\n",
    "as required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f3ae1a-6574-4f52-b0c9-c345e5363aa5",
   "metadata": {},
   "source": [
    "**Exercise 5.3** [Bayes factors and ROC curves *]\n",
    "Let $B = p(D|H_1)/p(D|H_0)$ be the Bayes factor in favor of model $1$. Suppose we plot two ROC curves, one computed by thresholding $B$, and the other computed by thresholding $p(H_1|D)$. Will they be the same or different? Explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75b9550-3240-4be0-bded-2d86be288ffb",
   "metadata": {},
   "source": [
    "We use a threshold to determine whether we accept model 1, and we consider the result to be \"True Positive\", etc., as:\n",
    "\n",
    "    Better       Model        \n",
    "    Model        Accepted     Result\n",
    "    \n",
    "    H0           H0           True Negative\n",
    "    H0           H1           False Positive\n",
    "    H1           H0           False Negative\n",
    "    H1           H1           True Positive\n",
    "    \n",
    "The unknown \"state of nature\", i.e., which is truly the better model, does not change whether we use the Bayes factor (which is the ratio of the marginal likelihoods associated with the two models) or the marginal likelihood for model $H_1$ to make the decision.  Assuming we use the Bayes Factor, we will follow this approach for a threshold $\\tau$:\n",
    "\n",
    "$B \\gt \\tau \\Rightarrow \\text{accept } H_1,$\n",
    "\n",
    "$B \\lt \\tau \\Rightarrow \\text{reject } H_1.$\n",
    "\n",
    "Since $B$ is the ratio of the two marginal likelihoods\n",
    "\n",
    "$B = p(D|H_1)/p(D|H_0),$ the Bayes Factor decision rule is equivalent to using the likelihood $p(D|H_1)$ and the modified decision rule:\n",
    "\n",
    "$p(D|H_1) \\gt p(D|H_0)\\tau \\Rightarrow \\text{accept } H_1,$\n",
    "\n",
    "$p(D|H_1) \\lt p(D|H_0)\\tau \\Rightarrow \\text{reject } H_1.$\n",
    "\n",
    "The TPR and FPR will then be identical for the two models when the threshold is appropriately scaled, and hence the resulting ROC will look identical for both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f68a0f-5b6e-4a1f-ba1b-1a8bb1796570",
   "metadata": {},
   "source": [
    "**Exercise 5.4** [Posterior median is optimal estimate under L1 loss] \n",
    "\n",
    "Prove that the posterior median is the optimal estimate under L1 loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da349567-2bce-4764-b5d2-56c382bc5822",
   "metadata": {},
   "source": [
    "The risk associated with the L1 loss \n",
    "\n",
    "$l_1(h, a) = |h-a|$\n",
    "\n",
    "is the expectation of the absolute difference between $h$ and $a$:\n",
    "\n",
    "$R(a|x) = \\mathbb{E}[|h-a||x] = \\int_{-\\infty}^{\\infty}|h-a|p(h|x)dh$\n",
    "\n",
    "$= \\int_{-\\infty}^{\\delta(h)}(a - \\delta(h))p(h|x)dh + \\int_{\\delta(h)}^{\\infty}(\\delta(h) - a)p(h|x)dh.$\n",
    "\n",
    "The optimal action $\\delta(h)$ will minimize the risk:\n",
    "\n",
    "$\\Rightarrow \\frac{d}{da}[R(a|x)] = \\int_{-\\infty}^{\\delta(h)}p(h|x)dh - \\int_{\\delta(h)}^{\\infty}p(h|x)dh = 0,$\n",
    "\n",
    "so that \n",
    "\n",
    "$\\int_{-\\infty}^{\\delta(h)}p(h|x)dh = \\int_{\\delta(h)}^{\\infty}p(h|x)dh,$\n",
    "\n",
    "which means that\n",
    "\n",
    "$2\\int_{-\\infty}^{\\delta(h)}p(h|x)dh = \\int_{-\\infty}^{\\infty}p(h|x)dh = 1,$\n",
    "\n",
    "so that\n",
    "\n",
    "$\\int_{-\\infty}^{\\delta(h)}p(h|x)dh =1,$\n",
    "\n",
    "meaning that $\\delta(h)$ must be the median of $p(h|x)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
